{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Up Data\n",
    "I'm going to clean up and filter the large dataset that I was given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13990410 entries, 0 to 13990409\n",
      "Data columns (total 8 columns):\n",
      " #   Column                    Dtype  \n",
      "---  ------                    -----  \n",
      " 0   fake_round_id             int64  \n",
      " 1   round_completion_seconds  int64  \n",
      " 2   round_skipped             bool   \n",
      " 3   black_card_text           object \n",
      " 4   black_card_pick_num       int64  \n",
      " 5   white_card_text           object \n",
      " 6   won                       bool   \n",
      " 7   winning_index             float64\n",
      "dtypes: bool(2), float64(1), int64(3), object(2)\n",
      "memory usage: 667.1+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/cah_2023.csv')\n",
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "First, I need to clean up the data. The global fixes are standardizing the apostrophe character, removing the '(PICK 2)' suffix, stripping the outer whitespace, and standardizing the blank space length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fake_round_id</th>\n",
       "      <th>round_completion_seconds</th>\n",
       "      <th>round_skipped</th>\n",
       "      <th>black_card_text</th>\n",
       "      <th>black_card_pick_num</th>\n",
       "      <th>white_card_text</th>\n",
       "      <th>won</th>\n",
       "      <th>winning_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "      <td>Hi MTV! My name is Kendra, I live in Malibu, I...</td>\n",
       "      <td>1</td>\n",
       "      <td>That chicken from Popeyes.®</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "      <td>Hi MTV! My name is Kendra, I live in Malibu, I...</td>\n",
       "      <td>1</td>\n",
       "      <td>Shapes and colors.</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "      <td>Hi MTV! My name is Kendra, I live in Malibu, I...</td>\n",
       "      <td>1</td>\n",
       "      <td>Mufasa's death scene.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "      <td>Hi MTV! My name is Kendra, I live in Malibu, I...</td>\n",
       "      <td>1</td>\n",
       "      <td>Going inside at some point because of the mosq...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "      <td>Hi MTV! My name is Kendra, I live in Malibu, I...</td>\n",
       "      <td>1</td>\n",
       "      <td>Chunky highlights.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13990405</th>\n",
       "      <td>1399041</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "      <td>Listen, Gary, I like you. But if you want that...</td>\n",
       "      <td>1</td>\n",
       "      <td>Regulatory capture.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13990406</th>\n",
       "      <td>1399041</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "      <td>Listen, Gary, I like you. But if you want that...</td>\n",
       "      <td>1</td>\n",
       "      <td>A Christmas feast of goose and jellies.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13990407</th>\n",
       "      <td>1399041</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "      <td>Listen, Gary, I like you. But if you want that...</td>\n",
       "      <td>1</td>\n",
       "      <td>Being mindful of cyclists.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13990408</th>\n",
       "      <td>1399041</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "      <td>Listen, Gary, I like you. But if you want that...</td>\n",
       "      <td>1</td>\n",
       "      <td>Feeding a three-course Italian dinner to a mai...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13990409</th>\n",
       "      <td>1399041</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "      <td>Listen, Gary, I like you. But if you want that...</td>\n",
       "      <td>1</td>\n",
       "      <td>Only dating Asian women.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13166720 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          fake_round_id  round_completion_seconds  round_skipped  \\\n",
       "0                     1                        24          False   \n",
       "1                     1                        24          False   \n",
       "2                     1                        24          False   \n",
       "3                     1                        24          False   \n",
       "4                     1                        24          False   \n",
       "...                 ...                       ...            ...   \n",
       "13990405        1399041                        19          False   \n",
       "13990406        1399041                        19          False   \n",
       "13990407        1399041                        19          False   \n",
       "13990408        1399041                        19          False   \n",
       "13990409        1399041                        19          False   \n",
       "\n",
       "                                            black_card_text  \\\n",
       "0         Hi MTV! My name is Kendra, I live in Malibu, I...   \n",
       "1         Hi MTV! My name is Kendra, I live in Malibu, I...   \n",
       "2         Hi MTV! My name is Kendra, I live in Malibu, I...   \n",
       "3         Hi MTV! My name is Kendra, I live in Malibu, I...   \n",
       "4         Hi MTV! My name is Kendra, I live in Malibu, I...   \n",
       "...                                                     ...   \n",
       "13990405  Listen, Gary, I like you. But if you want that...   \n",
       "13990406  Listen, Gary, I like you. But if you want that...   \n",
       "13990407  Listen, Gary, I like you. But if you want that...   \n",
       "13990408  Listen, Gary, I like you. But if you want that...   \n",
       "13990409  Listen, Gary, I like you. But if you want that...   \n",
       "\n",
       "          black_card_pick_num  \\\n",
       "0                           1   \n",
       "1                           1   \n",
       "2                           1   \n",
       "3                           1   \n",
       "4                           1   \n",
       "...                       ...   \n",
       "13990405                    1   \n",
       "13990406                    1   \n",
       "13990407                    1   \n",
       "13990408                    1   \n",
       "13990409                    1   \n",
       "\n",
       "                                            white_card_text    won  \\\n",
       "0                               That chicken from Popeyes.®  False   \n",
       "1                                        Shapes and colors.   True   \n",
       "2                                     Mufasa's death scene.  False   \n",
       "3         Going inside at some point because of the mosq...  False   \n",
       "4                                        Chunky highlights.  False   \n",
       "...                                                     ...    ...   \n",
       "13990405                                Regulatory capture.  False   \n",
       "13990406            A Christmas feast of goose and jellies.  False   \n",
       "13990407                         Being mindful of cyclists.  False   \n",
       "13990408  Feeding a three-course Italian dinner to a mai...  False   \n",
       "13990409                           Only dating Asian women.  False   \n",
       "\n",
       "          winning_index  \n",
       "0                   NaN  \n",
       "1                   0.0  \n",
       "2                   NaN  \n",
       "3                   NaN  \n",
       "4                   NaN  \n",
       "...                 ...  \n",
       "13990405            NaN  \n",
       "13990406            NaN  \n",
       "13990407            NaN  \n",
       "13990408            NaN  \n",
       "13990409            NaN  \n",
       "\n",
       "[13166720 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[data['round_skipped'] == False]\n",
    "data['black_card_text'] = data['black_card_text'].replace('’', '\\'', regex=True)\n",
    "data['white_card_text'] = data['white_card_text'].replace('’', '\\'', regex=True)\n",
    "data['black_card_text'] = data['black_card_text'].replace(' \\(PICK 2\\)', '', regex=True)\n",
    "data['black_card_text'] = data['black_card_text'].str.strip()\n",
    "data['white_card_text'] = data['white_card_text'].str.strip()\n",
    "data['black_card_text'] = data['black_card_text'].replace('_+', '___', regex=True)\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analyzing the black cards that show up less than 1000 times, I found several duplicates that were only off by a couple words or had differing punctuation or spelling. I took the more common text and used that for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fake_round_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>636.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2070.238994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1013.899433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2028.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2614.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2715.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3018.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fake_round_id\n",
       "count     636.000000\n",
       "mean     2070.238994\n",
       "std      1013.899433\n",
       "min         2.000000\n",
       "25%      2028.750000\n",
       "50%      2614.000000\n",
       "75%      2715.000000\n",
       "max      3018.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data['black_card_text'] = np.where(data['black_card_text'].str.contains('Randy'),\n",
    "    'Me: Hey, Randy! What\\'s going on?\\n\\nRandy: Aw nothing, man. Just _____.', data['black_card_text'])\n",
    "data['black_card_text'] = np.where(data['black_card_text'].str.contains('McDonalds'),\n",
    "    '_____ is back! Only at McDonalds.', data['black_card_text'])\n",
    "data['black_card_text'] = np.where(data['black_card_text'].str.contains('peaceful'),\n",
    "    'Join our peaceful community. We eat grapes. We live in tents. We enjoy _____, together.',\n",
    "    data['black_card_text'])\n",
    "data['black_card_text'] = np.where(data['black_card_text'].str.contains('click'),\n",
    "    'Your dreams are one click away! Learn more at _____.com.',\n",
    "    data['black_card_text'])\n",
    "data['black_card_text'] = np.where(data['black_card_text'].str.contains('diverged'),\n",
    "    'Two roads diverged in a yellow wood—\\nI chose _____,\\nAnd that has made all the difference.',\n",
    "    data['black_card_text'])\n",
    "data['black_card_text'] = np.where(data['black_card_text'].str.contains('Philadelphia'),\n",
    "    'And now, from WHYY in Philadelphia, it\\'s \"_____ with Terry Gross.\"',\n",
    "    data['black_card_text'])\n",
    "data['black_card_text'] = np.where(data['black_card_text'].str.contains('Death'),\n",
    "    'Because I could not stop for Death –\\nHe kindly stopped for me –\\nThe Carriage held but just Ourselves –\\nAnd _____.\\n\\n- Emily Dickinson',\n",
    "    data['black_card_text'])\n",
    "data['black_card_text'] = np.where(data['black_card_text'].str.contains('Guy'),\n",
    "    'Hey look everybody! It\\'s \"_____ Guy.\"',\n",
    "    data['black_card_text'])\n",
    "data['black_card_text'] = np.where(data['black_card_text'].str.contains('neighbor'),\n",
    "    'Howdy, neighbor! I couldn\\'t help but notice you struggling with _____. Need a hand?',\n",
    "    data['black_card_text'])\n",
    "data['black_card_text'] = np.where(data['black_card_text'].str.contains('I\\'m done with _____. It\\'s time for'),\n",
    "    'Alright, I\\'m done with _____. It\\'s time for _____.',\n",
    "    data['black_card_text'])\n",
    "data['black_card_text'] = np.where(data['black_card_text'].str.contains('Look at me'),\n",
    "    'Mom! Mom! Look at me, Mom! I\\'m _____!',\n",
    "    data['black_card_text'])\n",
    "data['black_card_text'] = np.where(data['black_card_text'].str.contains('please guide'),\n",
    "    'Lord, please guide my baby girl back to you and away from _____. Amen.',\n",
    "    data['black_card_text'])\n",
    "data['black_card_text'] = np.where(data['black_card_text'].str.contains('around back'),\n",
    "    'Pssst. You go around back. I\\'ll distract the guards with _____.',\n",
    "    data['black_card_text'])\n",
    "data['black_card_text'] = np.where(data['black_card_text'].str.contains('podcast'),\n",
    "    'Hey, check out my podcast! It\\'s just two regular guys talking about _____.',\n",
    "    data['black_card_text'])\n",
    "data['black_card_text'] = np.where(data['black_card_text'].str.contains('Tuesday'),\n",
    "    'Every Tuesday, I purchase a box of donuts. I sit on the toilet. I eat the donuts. I remember _____, and I cry.',\n",
    "    data['black_card_text'])\n",
    "data['black_card_text'] = np.where(data['black_card_text'].str.contains('THAT'),\n",
    "    'Holy MOLY! Now THAT\\'S what I call _____!',\n",
    "    data['black_card_text'])\n",
    "data['black_card_text'] = np.where(data['black_card_text'].str.contains('sex life'),\n",
    "    'Today\\'s #1 Tip: Spice up your sex life by bringing _____ into the bedroom!',\n",
    "    data['black_card_text'])\n",
    "data['black_card_text'] = np.where(data['black_card_text'].str.contains('grief'),\n",
    "    'In Irish culture, mourners express their grief through the traditional practice of _____.',\n",
    "    data['black_card_text'])\n",
    "data['black_card_text'] = np.where(data['black_card_text'].str.contains('drama'),\n",
    "    'Premiering tonight: NBC\\'s new heartfelt family drama, This Is _____.',\n",
    "    data['black_card_text'])\n",
    "data['black_card_text'] = np.where(data['black_card_text'].str.contains('Target'),\n",
    "    'Attention Target shoppers. Unfortunately, we will be closing early due to _____.',\n",
    "    data['black_card_text'])\n",
    "data['black_card_text'] = np.where(data['black_card_text'].str.contains('Steppenwolf'),\n",
    "    'This season at the Steppenwolf Theatre, Samuel Beckett\\'s classic existential play: Waiting for ___.',\n",
    "    data['black_card_text'])\n",
    "black_cards = data.drop_duplicates('fake_round_id').groupby('black_card_text')['fake_round_id'].count().reset_index()\n",
    "black_cards.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the white cards, a few of them had the swear words censored out with #. There were too many white cards to check, but I found a couple that had alternate spellings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['white_card_text'] = data['white_card_text'].replace(' ############ ', ' motherfucker ', regex=True)\n",
    "data['white_card_text'] = data['white_card_text'].replace(' ########', ' shittier', regex=True)\n",
    "data['white_card_text'] = data['white_card_text'].replace(' ####### ', ' fucking ', regex=True)\n",
    "data['white_card_text'] = data['white_card_text'].replace('####### ', 'Fucking ', regex=True)\n",
    "data['white_card_text'] = data['white_card_text'].replace(' #####', ' bitch', regex=True)\n",
    "data['white_card_text'] = data['white_card_text'].replace(' ####', ' shit', regex=True)\n",
    "data['white_card_text'] = data['white_card_text'].replace(' ### ', ' ass ', regex=True)\n",
    "\n",
    "data['white_card_text'] = np.where(data['white_card_text'].str.contains('football players'),\n",
    "    '10 football players with erections barreling towards you at full speed.',\n",
    "    data['white_card_text'])\n",
    "data['white_card_text'] = np.where(data['white_card_text'].str.contains('black-tar'),\n",
    "    '8 oz. of Mexican black-tar heroin.',\n",
    "    data['white_card_text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is cleaned, I'll save this to its own csv so I don't have to rerun the steps above again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data/cah_2023_clean.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter\n",
    "There are over 13 million records and 1 million rounds. We'll trim down the data to have a manageable amount. The fastest 25% of rounds will be removed since those could have been picked randomly. This comes out to 9 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('data/cah_2023_clean.csv')\n",
    "data = data[data['round_completion_seconds'] >= 9]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll save all of the rounds that require 2 white cards as a response in its own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "double = data[data['black_card_pick_num'] == 2]\n",
    "double.to_csv('data/double_card_rounds.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I only took the rounds where the white cards showed up in at least 3,500 rounds and the black cards showed up in 1,200 rounds. This resulted in about 6 million records (or 600,000 rounds) which I used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = data[data['black_card_pick_num'] == 1]\n",
    "unq, count = np.unique(data['white_card_text'], return_counts=True)\n",
    "white_cards = unq[count < 3500]\n",
    "remove_rounds = data[data['white_card_text'].isin(white_cards)]['fake_round_id'].unique()\n",
    "filtered = data[~data['fake_round_id'].isin(remove_rounds)]\n",
    "\n",
    "unq, count = np.unique(data['black_card_text'], return_counts=True)\n",
    "black_cards = unq[count > 12000]\n",
    "filtered = filtered[filtered['black_card_text'].isin(black_cards)]\n",
    "\n",
    "filtered.info()\n",
    "filtered.to_csv('data/cah_2023_combined.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I took the inverse of the filtering above to use as a dataset with new cards that the model hadn't seen. I decreased the white card threshold to 3,100 to make the number of rounds smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = data[data['black_card_pick_num'] == 1]\n",
    "unq, count = np.unique(data['white_card_text'], return_counts=True)\n",
    "white_cards = unq[count < 3100]\n",
    "rare_rounds = data[data['white_card_text'].isin(white_cards)]['fake_round_id'].unique()\n",
    "filtered = data[data['fake_round_id'].isin(rare_rounds)]\n",
    "\n",
    "unq, count = np.unique(data['black_card_text'], return_counts=True)\n",
    "black_cards = unq[count > 12000]\n",
    "filtered = filtered[~filtered['black_card_text'].isin(black_cards)]\n",
    "\n",
    "filtered.to_csv('data/cah_rare_cards.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface Data\n",
    "For the interface, I just need a list of all the unique white and black cards. I didn't care if they were double cards or rarely used. I included everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('data/cah_2023_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_cards = data['black_card_text'].unique().tolist()\n",
    "white_cards = data['white_card_text'].unique().tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I saved this to a JSON file for easy reading. It was formatted nicely, so I could easily remove cards that were inappropriate for presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data/data.json', 'w') as f:\n",
    "    f.write('{\\n')\n",
    "    f.write('\\t\"black\": [\\n')\n",
    "    for i in black_cards:\n",
    "        f.write('\\t\\t' + json.dumps(i) + ',\\n')\n",
    "    f.write('\\t],\\n')\n",
    "    f.write('\\t\"white\": [\\n')\n",
    "    for i in white_cards:\n",
    "        f.write('\\t\\t' + json.dumps(i) + ',\\n')\n",
    "    f.write('\\t]\\n')\n",
    "    f.write('}')\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs536nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
